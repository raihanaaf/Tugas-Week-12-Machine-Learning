{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XASRsS4tkt8B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Transformasi Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "Sj9fi0anmBNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Fashion MNIST Dataset\n",
        "batch_size = 32\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O46bZ-PAmD4g",
        "outputId": "e8bcc299-011f-407c-ff8b-1d7ae071cb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 21.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 342kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 16.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset menjadi train dan validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "VDOtWbJFmJyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definisi Arsitektur CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling_type='max'):\n",
        "        super(CNN, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=kernel_size, padding=padding)  # Input 1 channel (grayscale)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pooling_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Size of feature map after pooling: 7x7\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes in Fashion MNIST\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0KXrEdwzmN2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training dan Evaluasi\n",
        "def train_and_evaluate(kernel_size, pooling_type, optimizer_type, epochs, early_stopping_patience=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = CNN(kernel_size=kernel_size, pooling_type=pooling_type).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Evaluating with params: {{'epochs': {epochs}, 'kernel_size': {kernel_size}, 'optimizer_type': '{optimizer_type}', 'pooling_type': '{pooling_type}'}}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Step\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation Step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early Stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Evaluasi Akurasi pada Test Dataset\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# 5. Eksperimen dengan Semua Kombinasi Parameter\n",
        "kernel_sizes = [3, 5, 7]\n",
        "pooling_types = ['max', 'avg']\n",
        "optimizers = ['SGD', 'RMSProp', 'Adam']\n",
        "epoch_list = [5, 50, 100, 250, 350]\n",
        "\n",
        "results = []\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    for pooling_type in pooling_types:\n",
        "        for optimizer in optimizers:\n",
        "            for epochs in epoch_list:\n",
        "                print(f\"Running experiment with kernel_size={kernel_size}, pooling_type={pooling_type}, optimizer={optimizer}, epochs={epochs}\")\n",
        "                accuracy = train_and_evaluate(kernel_size, pooling_type, optimizer, epochs, early_stopping_patience=10)\n",
        "                results.append((kernel_size, pooling_type, optimizer, epochs, accuracy))\n",
        "\n",
        "# Menampilkan Hasil Akhir\n",
        "print(\"\\nHasil Akhir Eksperimen:\")\n",
        "for result in results:\n",
        "    print(f\"Kernel Size: {result[0]}, Pooling: {result[1]}, Optimizer: {result[2]}, Epochs: {result[3]}, Accuracy: {result[4]:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xhd79y-WmRqF",
        "outputId": "6af1a3bc-fec5-48a2-c08e-5263df8f0708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6399, Val Loss: 0.3694\n",
            "Epoch 2/5, Loss: 0.3816, Val Loss: 0.3022\n",
            "Epoch 3/5, Loss: 0.3211, Val Loss: 0.2711\n",
            "Epoch 4/5, Loss: 0.2856, Val Loss: 0.2663\n",
            "Epoch 5/5, Loss: 0.2603, Val Loss: 0.2553\n",
            "Accuracy: 89.67%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 0.6120, Val Loss: 0.3609\n",
            "Epoch 2/50, Loss: 0.3817, Val Loss: 0.3052\n",
            "Epoch 3/50, Loss: 0.3236, Val Loss: 0.2824\n",
            "Epoch 4/50, Loss: 0.2867, Val Loss: 0.2675\n",
            "Epoch 5/50, Loss: 0.2611, Val Loss: 0.2434\n",
            "Epoch 6/50, Loss: 0.2401, Val Loss: 0.2438\n",
            "Epoch 7/50, Loss: 0.2225, Val Loss: 0.2379\n",
            "Epoch 8/50, Loss: 0.2098, Val Loss: 0.2356\n",
            "Epoch 9/50, Loss: 0.1949, Val Loss: 0.2387\n",
            "Epoch 10/50, Loss: 0.1818, Val Loss: 0.2472\n",
            "Epoch 11/50, Loss: 0.1765, Val Loss: 0.2292\n",
            "Epoch 12/50, Loss: 0.1660, Val Loss: 0.2521\n",
            "Epoch 13/50, Loss: 0.1550, Val Loss: 0.2270\n",
            "Epoch 14/50, Loss: 0.1473, Val Loss: 0.2468\n",
            "Epoch 15/50, Loss: 0.1398, Val Loss: 0.2583\n",
            "Epoch 16/50, Loss: 0.1344, Val Loss: 0.2495\n",
            "Epoch 17/50, Loss: 0.1275, Val Loss: 0.2647\n",
            "Epoch 18/50, Loss: 0.1221, Val Loss: 0.2614\n",
            "Epoch 19/50, Loss: 0.1177, Val Loss: 0.2642\n",
            "Epoch 20/50, Loss: 0.0858, Val Loss: 0.2655\n",
            "Epoch 21/50, Loss: 0.0730, Val Loss: 0.2785\n",
            "Epoch 22/50, Loss: 0.0703, Val Loss: 0.2942\n",
            "Epoch 23/50, Loss: 0.0653, Val Loss: 0.3042\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.30%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 0.6115, Val Loss: 0.3677\n",
            "Epoch 2/100, Loss: 0.3797, Val Loss: 0.3150\n",
            "Epoch 3/100, Loss: 0.3190, Val Loss: 0.2857\n",
            "Epoch 4/100, Loss: 0.2794, Val Loss: 0.2548\n",
            "Epoch 5/100, Loss: 0.2570, Val Loss: 0.2681\n",
            "Epoch 6/100, Loss: 0.2370, Val Loss: 0.2475\n",
            "Epoch 7/100, Loss: 0.2218, Val Loss: 0.2320\n",
            "Epoch 8/100, Loss: 0.2071, Val Loss: 0.2303\n",
            "Epoch 9/100, Loss: 0.1940, Val Loss: 0.2397\n",
            "Epoch 10/100, Loss: 0.1829, Val Loss: 0.2296\n",
            "Epoch 11/100, Loss: 0.1718, Val Loss: 0.2415\n",
            "Epoch 12/100, Loss: 0.1609, Val Loss: 0.2362\n",
            "Epoch 13/100, Loss: 0.1569, Val Loss: 0.2523\n",
            "Epoch 14/100, Loss: 0.1473, Val Loss: 0.2468\n",
            "Epoch 15/100, Loss: 0.1422, Val Loss: 0.2397\n",
            "Epoch 16/100, Loss: 0.1325, Val Loss: 0.2791\n",
            "Epoch 17/100, Loss: 0.0999, Val Loss: 0.2521\n",
            "Epoch 18/100, Loss: 0.0850, Val Loss: 0.2630\n",
            "Epoch 19/100, Loss: 0.0782, Val Loss: 0.2744\n",
            "Epoch 20/100, Loss: 0.0746, Val Loss: 0.2810\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.17%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 0.6694, Val Loss: 0.3767\n",
            "Epoch 2/250, Loss: 0.4121, Val Loss: 0.3137\n",
            "Epoch 3/250, Loss: 0.3439, Val Loss: 0.2763\n",
            "Epoch 4/250, Loss: 0.3043, Val Loss: 0.2670\n",
            "Epoch 5/250, Loss: 0.2721, Val Loss: 0.2430\n",
            "Epoch 6/250, Loss: 0.2522, Val Loss: 0.2436\n",
            "Epoch 7/250, Loss: 0.2338, Val Loss: 0.2524\n",
            "Epoch 8/250, Loss: 0.2210, Val Loss: 0.2373\n",
            "Epoch 9/250, Loss: 0.2069, Val Loss: 0.2423\n",
            "Epoch 10/250, Loss: 0.1927, Val Loss: 0.2448\n",
            "Epoch 11/250, Loss: 0.1855, Val Loss: 0.2336\n",
            "Epoch 12/250, Loss: 0.1726, Val Loss: 0.2535\n",
            "Epoch 13/250, Loss: 0.1638, Val Loss: 0.2453\n",
            "Epoch 14/250, Loss: 0.1567, Val Loss: 0.2480\n",
            "Epoch 15/250, Loss: 0.1515, Val Loss: 0.2592\n",
            "Epoch 16/250, Loss: 0.1395, Val Loss: 0.2463\n",
            "Epoch 17/250, Loss: 0.1346, Val Loss: 0.2607\n",
            "Epoch 18/250, Loss: 0.1034, Val Loss: 0.2752\n",
            "Epoch 19/250, Loss: 0.0906, Val Loss: 0.2892\n",
            "Epoch 20/250, Loss: 0.0833, Val Loss: 0.2957\n",
            "Epoch 21/250, Loss: 0.0758, Val Loss: 0.2932\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.05%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 0.6343, Val Loss: 0.3933\n",
            "Epoch 2/350, Loss: 0.3893, Val Loss: 0.2976\n",
            "Epoch 3/350, Loss: 0.3277, Val Loss: 0.2750\n",
            "Epoch 4/350, Loss: 0.2887, Val Loss: 0.2570\n",
            "Epoch 5/350, Loss: 0.2627, Val Loss: 0.2386\n",
            "Epoch 6/350, Loss: 0.2437, Val Loss: 0.2543\n",
            "Epoch 7/350, Loss: 0.2261, Val Loss: 0.2424\n",
            "Epoch 8/350, Loss: 0.2117, Val Loss: 0.2351\n",
            "Epoch 9/350, Loss: 0.1988, Val Loss: 0.2588\n",
            "Epoch 10/350, Loss: 0.1862, Val Loss: 0.2445\n",
            "Epoch 11/350, Loss: 0.1765, Val Loss: 0.2381\n",
            "Epoch 12/350, Loss: 0.1686, Val Loss: 0.2459\n",
            "Epoch 13/350, Loss: 0.1594, Val Loss: 0.2442\n",
            "Epoch 14/350, Loss: 0.1506, Val Loss: 0.2479\n",
            "Epoch 15/350, Loss: 0.1153, Val Loss: 0.2368\n",
            "Epoch 16/350, Loss: 0.1027, Val Loss: 0.2447\n",
            "Epoch 17/350, Loss: 0.0982, Val Loss: 0.2627\n",
            "Epoch 18/350, Loss: 0.0892, Val Loss: 0.2658\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.27%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/5, Loss: 2.4671, Val Loss: 0.4880\n",
            "Epoch 2/5, Loss: 0.5958, Val Loss: 0.4237\n",
            "Epoch 3/5, Loss: 0.5436, Val Loss: 0.4258\n",
            "Epoch 4/5, Loss: 0.5281, Val Loss: 0.4128\n",
            "Epoch 5/5, Loss: 0.5350, Val Loss: 0.4185\n",
            "Accuracy: 84.91%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 2.4082, Val Loss: 0.5006\n",
            "Epoch 2/50, Loss: 0.6165, Val Loss: 0.4829\n",
            "Epoch 3/50, Loss: 0.5894, Val Loss: 0.4510\n",
            "Epoch 4/50, Loss: 0.6161, Val Loss: 0.4819\n",
            "Epoch 5/50, Loss: 0.5453, Val Loss: 0.4485\n",
            "Epoch 6/50, Loss: 0.5502, Val Loss: 0.4405\n",
            "Epoch 7/50, Loss: 0.5439, Val Loss: 0.4334\n",
            "Epoch 8/50, Loss: 0.5323, Val Loss: 0.4340\n",
            "Epoch 9/50, Loss: 0.5242, Val Loss: 0.4330\n",
            "Epoch 10/50, Loss: 0.5180, Val Loss: 0.4420\n",
            "Epoch 11/50, Loss: 0.5249, Val Loss: 0.4627\n",
            "Epoch 12/50, Loss: 0.5208, Val Loss: 0.4431\n",
            "Epoch 13/50, Loss: 0.5267, Val Loss: 0.4434\n",
            "Epoch 14/50, Loss: 0.5233, Val Loss: 0.4686\n",
            "Epoch 15/50, Loss: 0.5288, Val Loss: 0.4397\n",
            "Epoch 16/50, Loss: 0.4554, Val Loss: 0.3894\n",
            "Epoch 17/50, Loss: 0.4492, Val Loss: 0.3889\n",
            "Epoch 18/50, Loss: 0.4440, Val Loss: 0.3991\n",
            "Epoch 19/50, Loss: 0.4464, Val Loss: 0.3810\n",
            "Epoch 20/50, Loss: 0.4397, Val Loss: 0.4027\n",
            "Epoch 21/50, Loss: 0.4399, Val Loss: 0.4018\n",
            "Epoch 22/50, Loss: 0.4469, Val Loss: 0.4064\n",
            "Epoch 23/50, Loss: 0.4402, Val Loss: 0.4014\n",
            "Epoch 24/50, Loss: 0.4377, Val Loss: 0.4104\n",
            "Epoch 25/50, Loss: 0.4399, Val Loss: 0.3932\n",
            "Epoch 26/50, Loss: 0.4001, Val Loss: 0.3746\n",
            "Epoch 27/50, Loss: 0.3936, Val Loss: 0.3761\n",
            "Epoch 28/50, Loss: 0.3965, Val Loss: 0.3613\n",
            "Epoch 29/50, Loss: 0.3909, Val Loss: 0.4052\n",
            "Epoch 30/50, Loss: 0.3970, Val Loss: 0.3565\n",
            "Epoch 31/50, Loss: 0.3926, Val Loss: 0.3573\n",
            "Epoch 32/50, Loss: 0.3957, Val Loss: 0.4141\n",
            "Epoch 33/50, Loss: 0.3944, Val Loss: 0.3656\n",
            "Epoch 34/50, Loss: 0.3952, Val Loss: 0.3661\n",
            "Epoch 35/50, Loss: 0.3926, Val Loss: 0.3677\n",
            "Epoch 36/50, Loss: 0.3963, Val Loss: 0.3787\n",
            "Epoch 37/50, Loss: 0.3688, Val Loss: 0.3469\n",
            "Epoch 38/50, Loss: 0.3696, Val Loss: 0.3500\n",
            "Epoch 39/50, Loss: 0.3692, Val Loss: 0.3455\n",
            "Epoch 40/50, Loss: 0.3691, Val Loss: 0.3521\n",
            "Epoch 41/50, Loss: 0.3663, Val Loss: 0.3466\n",
            "Epoch 42/50, Loss: 0.3634, Val Loss: 0.3428\n",
            "Epoch 43/50, Loss: 0.3671, Val Loss: 0.3417\n",
            "Epoch 44/50, Loss: 0.3649, Val Loss: 0.3483\n",
            "Epoch 45/50, Loss: 0.3646, Val Loss: 0.3477\n",
            "Epoch 46/50, Loss: 0.3634, Val Loss: 0.3558\n",
            "Epoch 47/50, Loss: 0.3605, Val Loss: 0.3411\n",
            "Epoch 48/50, Loss: 0.3644, Val Loss: 0.3427\n",
            "Epoch 49/50, Loss: 0.3647, Val Loss: 0.3465\n",
            "Epoch 50/50, Loss: 0.3632, Val Loss: 0.3418\n",
            "Accuracy: 87.43%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 2.7871, Val Loss: 2.3039\n",
            "Epoch 2/100, Loss: 2.3041, Val Loss: 2.3053\n",
            "Epoch 3/100, Loss: 2.3040, Val Loss: 2.3038\n",
            "Epoch 4/100, Loss: 2.3040, Val Loss: 2.3050\n",
            "Epoch 5/100, Loss: 2.3041, Val Loss: 2.3039\n",
            "Epoch 6/100, Loss: 2.3039, Val Loss: 2.3043\n",
            "Epoch 7/100, Loss: 2.3040, Val Loss: 2.3053\n",
            "Epoch 8/100, Loss: 2.3032, Val Loss: 2.3034\n",
            "Epoch 9/100, Loss: 2.3034, Val Loss: 2.3033\n",
            "Epoch 10/100, Loss: 2.3033, Val Loss: 2.3036\n",
            "Epoch 11/100, Loss: 2.3033, Val Loss: 2.3034\n",
            "Epoch 12/100, Loss: 2.3034, Val Loss: 2.3035\n",
            "Epoch 13/100, Loss: 2.3033, Val Loss: 2.3034\n",
            "Epoch 14/100, Loss: 2.3033, Val Loss: 2.3029\n",
            "Epoch 15/100, Loss: 2.3034, Val Loss: 2.3039\n",
            "Epoch 16/100, Loss: 2.3035, Val Loss: 2.3030\n",
            "Epoch 17/100, Loss: 2.3032, Val Loss: 2.3029\n",
            "Epoch 18/100, Loss: 2.3034, Val Loss: 2.3039\n",
            "Epoch 19/100, Loss: 2.3032, Val Loss: 2.3035\n",
            "Epoch 20/100, Loss: 2.3033, Val Loss: 2.3036\n",
            "Epoch 21/100, Loss: 2.3031, Val Loss: 2.3033\n",
            "Epoch 22/100, Loss: 2.3030, Val Loss: 2.3035\n",
            "Epoch 23/100, Loss: 2.3029, Val Loss: 2.3032\n",
            "Epoch 24/100, Loss: 2.3030, Val Loss: 2.3029\n",
            "Epoch 25/100, Loss: 2.3030, Val Loss: 2.3029\n",
            "Epoch 26/100, Loss: 2.3030, Val Loss: 2.3029\n",
            "Epoch 27/100, Loss: 2.3028, Val Loss: 2.3030\n",
            "Epoch 28/100, Loss: 2.3028, Val Loss: 2.3027\n",
            "Epoch 29/100, Loss: 2.3028, Val Loss: 2.3027\n",
            "Epoch 30/100, Loss: 2.3028, Val Loss: 2.3029\n",
            "Epoch 31/100, Loss: 2.3028, Val Loss: 2.3026\n",
            "Epoch 32/100, Loss: 2.3028, Val Loss: 2.3029\n",
            "Epoch 33/100, Loss: 2.3028, Val Loss: 2.3030\n",
            "Epoch 34/100, Loss: 2.3028, Val Loss: 2.3029\n",
            "Epoch 35/100, Loss: 2.3027, Val Loss: 2.3027\n",
            "Epoch 36/100, Loss: 2.3027, Val Loss: 2.3027\n",
            "Epoch 37/100, Loss: 2.3027, Val Loss: 2.3027\n",
            "Epoch 38/100, Loss: 2.3027, Val Loss: 2.3028\n",
            "Epoch 39/100, Loss: 2.3027, Val Loss: 2.3028\n",
            "Epoch 40/100, Loss: 2.3027, Val Loss: 2.3028\n",
            "Epoch 41/100, Loss: 2.3026, Val Loss: 2.3027\n",
            "Early stopping triggered.\n",
            "Accuracy: 10.00%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 1.6793, Val Loss: 0.4578\n",
            "Epoch 2/250, Loss: 0.5798, Val Loss: 0.4512\n",
            "Epoch 3/250, Loss: 0.5277, Val Loss: 0.4493\n",
            "Epoch 4/250, Loss: 0.5322, Val Loss: 0.4273\n",
            "Epoch 5/250, Loss: 0.5054, Val Loss: 0.4966\n",
            "Epoch 6/250, Loss: 0.5052, Val Loss: 0.4208\n",
            "Epoch 7/250, Loss: 0.4976, Val Loss: 0.4086\n",
            "Epoch 8/250, Loss: 0.4921, Val Loss: 0.4323\n",
            "Epoch 9/250, Loss: 0.5087, Val Loss: 0.4289\n",
            "Epoch 10/250, Loss: 0.5620, Val Loss: 0.4300\n",
            "Epoch 11/250, Loss: 0.5409, Val Loss: 0.4260\n",
            "Epoch 12/250, Loss: 0.5193, Val Loss: 0.4529\n",
            "Epoch 13/250, Loss: 0.5038, Val Loss: 0.5551\n",
            "Epoch 14/250, Loss: 0.4456, Val Loss: 0.3899\n",
            "Epoch 15/250, Loss: 0.4419, Val Loss: 0.3878\n",
            "Epoch 16/250, Loss: 0.4423, Val Loss: 0.3988\n",
            "Epoch 17/250, Loss: 0.4276, Val Loss: 0.3907\n",
            "Epoch 18/250, Loss: 0.4287, Val Loss: 0.3949\n",
            "Epoch 19/250, Loss: 0.4319, Val Loss: 0.3991\n",
            "Epoch 20/250, Loss: 0.4287, Val Loss: 0.4276\n",
            "Epoch 21/250, Loss: 0.4356, Val Loss: 0.4572\n",
            "Epoch 22/250, Loss: 0.3917, Val Loss: 0.3926\n",
            "Epoch 23/250, Loss: 0.3800, Val Loss: 0.3835\n",
            "Epoch 24/250, Loss: 0.3793, Val Loss: 0.3639\n",
            "Epoch 25/250, Loss: 0.3792, Val Loss: 0.3601\n",
            "Epoch 26/250, Loss: 0.3785, Val Loss: 0.3628\n",
            "Epoch 27/250, Loss: 0.3764, Val Loss: 0.3634\n",
            "Epoch 28/250, Loss: 0.3784, Val Loss: 0.3634\n",
            "Epoch 29/250, Loss: 0.3777, Val Loss: 0.3815\n",
            "Epoch 30/250, Loss: 0.3752, Val Loss: 0.3623\n",
            "Epoch 31/250, Loss: 0.3774, Val Loss: 0.3584\n",
            "Epoch 32/250, Loss: 0.3755, Val Loss: 0.3703\n",
            "Epoch 33/250, Loss: 0.3759, Val Loss: 0.3549\n",
            "Epoch 34/250, Loss: 0.3756, Val Loss: 0.3858\n",
            "Epoch 35/250, Loss: 0.3731, Val Loss: 0.3416\n",
            "Epoch 36/250, Loss: 0.3735, Val Loss: 0.3598\n",
            "Epoch 37/250, Loss: 0.3719, Val Loss: 0.3643\n",
            "Epoch 38/250, Loss: 0.3722, Val Loss: 0.3485\n",
            "Epoch 39/250, Loss: 0.3715, Val Loss: 0.3899\n",
            "Epoch 40/250, Loss: 0.3744, Val Loss: 0.3619\n",
            "Epoch 41/250, Loss: 0.3777, Val Loss: 0.3681\n",
            "Epoch 42/250, Loss: 0.3524, Val Loss: 0.3457\n",
            "Epoch 43/250, Loss: 0.3419, Val Loss: 0.3408\n",
            "Epoch 44/250, Loss: 0.3427, Val Loss: 0.3493\n",
            "Epoch 45/250, Loss: 0.3386, Val Loss: 0.3353\n",
            "Epoch 46/250, Loss: 0.3404, Val Loss: 0.3488\n",
            "Epoch 47/250, Loss: 0.3400, Val Loss: 0.3337\n",
            "Epoch 48/250, Loss: 0.3423, Val Loss: 0.3498\n",
            "Epoch 49/250, Loss: 0.3435, Val Loss: 0.3386\n",
            "Epoch 50/250, Loss: 0.3417, Val Loss: 0.3510\n",
            "Epoch 51/250, Loss: 0.3402, Val Loss: 0.3397\n",
            "Epoch 52/250, Loss: 0.3410, Val Loss: 0.3415\n",
            "Epoch 53/250, Loss: 0.3380, Val Loss: 0.3381\n",
            "Epoch 54/250, Loss: 0.3262, Val Loss: 0.3462\n",
            "Epoch 55/250, Loss: 0.3243, Val Loss: 0.3297\n",
            "Epoch 56/250, Loss: 0.3242, Val Loss: 0.3296\n",
            "Epoch 57/250, Loss: 0.3249, Val Loss: 0.3377\n",
            "Epoch 58/250, Loss: 0.3221, Val Loss: 0.3425\n",
            "Epoch 59/250, Loss: 0.3261, Val Loss: 0.3234\n",
            "Epoch 60/250, Loss: 0.3200, Val Loss: 0.3418\n",
            "Epoch 61/250, Loss: 0.3204, Val Loss: 0.3255\n",
            "Epoch 62/250, Loss: 0.3202, Val Loss: 0.3239\n",
            "Epoch 63/250, Loss: 0.3213, Val Loss: 0.3228\n",
            "Epoch 64/250, Loss: 0.3201, Val Loss: 0.3339\n",
            "Epoch 65/250, Loss: 0.3232, Val Loss: 0.3284\n",
            "Epoch 66/250, Loss: 0.3218, Val Loss: 0.3232\n",
            "Epoch 67/250, Loss: 0.3204, Val Loss: 0.3268\n",
            "Epoch 68/250, Loss: 0.3201, Val Loss: 0.3750\n",
            "Epoch 69/250, Loss: 0.3230, Val Loss: 0.3512\n",
            "Epoch 70/250, Loss: 0.3123, Val Loss: 0.3308\n",
            "Epoch 71/250, Loss: 0.3120, Val Loss: 0.3198\n",
            "Epoch 72/250, Loss: 0.3115, Val Loss: 0.3253\n",
            "Epoch 73/250, Loss: 0.3104, Val Loss: 0.3403\n",
            "Epoch 74/250, Loss: 0.3094, Val Loss: 0.3280\n",
            "Epoch 75/250, Loss: 0.3100, Val Loss: 0.3188\n",
            "Epoch 76/250, Loss: 0.3076, Val Loss: 0.3200\n",
            "Epoch 77/250, Loss: 0.3124, Val Loss: 0.3289\n",
            "Epoch 78/250, Loss: 0.3091, Val Loss: 0.3330\n",
            "Epoch 79/250, Loss: 0.3076, Val Loss: 0.3245\n",
            "Epoch 80/250, Loss: 0.3093, Val Loss: 0.3231\n",
            "Epoch 81/250, Loss: 0.3095, Val Loss: 0.3231\n",
            "Epoch 82/250, Loss: 0.3044, Val Loss: 0.3276\n",
            "Epoch 83/250, Loss: 0.3054, Val Loss: 0.3218\n",
            "Epoch 84/250, Loss: 0.3034, Val Loss: 0.3217\n",
            "Epoch 85/250, Loss: 0.3053, Val Loss: 0.3216\n",
            "Early stopping triggered.\n",
            "Accuracy: 88.82%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 2.5445, Val Loss: 0.5534\n",
            "Epoch 2/350, Loss: 0.7519, Val Loss: 0.6186\n",
            "Epoch 3/350, Loss: 0.6174, Val Loss: 0.4768\n",
            "Epoch 4/350, Loss: 0.6058, Val Loss: 0.4626\n",
            "Epoch 5/350, Loss: 0.5627, Val Loss: 0.4895\n",
            "Epoch 6/350, Loss: 0.5538, Val Loss: 0.4576\n",
            "Epoch 7/350, Loss: 0.5403, Val Loss: 0.5802\n",
            "Epoch 8/350, Loss: 0.5340, Val Loss: 0.4473\n",
            "Epoch 9/350, Loss: 0.5312, Val Loss: 0.4533\n",
            "Epoch 10/350, Loss: 0.5673, Val Loss: 0.6130\n",
            "Epoch 11/350, Loss: 0.5291, Val Loss: 0.4460\n",
            "Epoch 12/350, Loss: 0.5413, Val Loss: 0.4751\n",
            "Epoch 13/350, Loss: 0.5371, Val Loss: 0.4475\n",
            "Epoch 14/350, Loss: 0.5348, Val Loss: 0.4291\n",
            "Epoch 15/350, Loss: 0.5522, Val Loss: 0.4529\n",
            "Epoch 16/350, Loss: 0.5314, Val Loss: 0.4470\n",
            "Epoch 17/350, Loss: 0.5416, Val Loss: 0.4985\n",
            "Epoch 18/350, Loss: 0.5572, Val Loss: 0.4758\n",
            "Epoch 19/350, Loss: 0.5327, Val Loss: 0.4577\n",
            "Epoch 20/350, Loss: 0.5782, Val Loss: 0.5068\n",
            "Epoch 21/350, Loss: 0.5274, Val Loss: 0.4360\n",
            "Epoch 22/350, Loss: 0.5123, Val Loss: 0.4451\n",
            "Epoch 23/350, Loss: 0.5070, Val Loss: 0.4542\n",
            "Epoch 24/350, Loss: 0.5011, Val Loss: 0.4326\n",
            "Early stopping triggered.\n",
            "Accuracy: 82.93%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=Adam, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'Adam', 'pooling_type': 'max'}\n",
            "Epoch 1/5, Loss: 0.5473, Val Loss: 0.3430\n",
            "Epoch 2/5, Loss: 0.3646, Val Loss: 0.2858\n",
            "Epoch 3/5, Loss: 0.3105, Val Loss: 0.2611\n",
            "Epoch 4/5, Loss: 0.2755, Val Loss: 0.2560\n",
            "Epoch 5/5, Loss: 0.2546, Val Loss: 0.2463\n",
            "Accuracy: 90.75%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=Adam, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'Adam', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 0.5488, Val Loss: 0.3313\n",
            "Epoch 2/50, Loss: 0.3637, Val Loss: 0.2934\n",
            "Epoch 3/50, Loss: 0.3072, Val Loss: 0.2605\n",
            "Epoch 4/50, Loss: 0.2763, Val Loss: 0.2466\n",
            "Epoch 5/50, Loss: 0.2531, Val Loss: 0.2529\n",
            "Epoch 6/50, Loss: 0.2339, Val Loss: 0.2284\n",
            "Epoch 7/50, Loss: 0.2163, Val Loss: 0.2379\n",
            "Epoch 8/50, Loss: 0.2000, Val Loss: 0.2339\n",
            "Epoch 9/50, Loss: 0.1870, Val Loss: 0.2336\n",
            "Epoch 10/50, Loss: 0.1774, Val Loss: 0.2293\n",
            "Epoch 11/50, Loss: 0.1662, Val Loss: 0.2366\n",
            "Epoch 12/50, Loss: 0.1579, Val Loss: 0.2585\n",
            "Epoch 13/50, Loss: 0.1277, Val Loss: 0.2463\n",
            "Epoch 14/50, Loss: 0.1148, Val Loss: 0.2479\n",
            "Epoch 15/50, Loss: 0.1074, Val Loss: 0.2667\n",
            "Epoch 16/50, Loss: 0.1016, Val Loss: 0.2786\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.02%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=Adam, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'Adam', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 0.5425, Val Loss: 0.3364\n",
            "Epoch 2/100, Loss: 0.3564, Val Loss: 0.2767\n",
            "Epoch 3/100, Loss: 0.3026, Val Loss: 0.2694\n",
            "Epoch 4/100, Loss: 0.2760, Val Loss: 0.2507\n",
            "Epoch 5/100, Loss: 0.2465, Val Loss: 0.2416\n",
            "Epoch 6/100, Loss: 0.2301, Val Loss: 0.2285\n",
            "Epoch 7/100, Loss: 0.2141, Val Loss: 0.2441\n",
            "Epoch 8/100, Loss: 0.1998, Val Loss: 0.2234\n",
            "Epoch 9/100, Loss: 0.1886, Val Loss: 0.2285\n",
            "Epoch 10/100, Loss: 0.1737, Val Loss: 0.2444\n",
            "Epoch 11/100, Loss: 0.1661, Val Loss: 0.2294\n",
            "Epoch 12/100, Loss: 0.1553, Val Loss: 0.2471\n",
            "Epoch 13/100, Loss: 0.1477, Val Loss: 0.2550\n",
            "Epoch 14/100, Loss: 0.1409, Val Loss: 0.2513\n",
            "Epoch 15/100, Loss: 0.1140, Val Loss: 0.2557\n",
            "Epoch 16/100, Loss: 0.1004, Val Loss: 0.2681\n",
            "Epoch 17/100, Loss: 0.0943, Val Loss: 0.2781\n",
            "Epoch 18/100, Loss: 0.0885, Val Loss: 0.2899\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.41%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=Adam, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'Adam', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 0.5302, Val Loss: 0.3260\n",
            "Epoch 2/250, Loss: 0.3449, Val Loss: 0.2843\n",
            "Epoch 3/250, Loss: 0.2911, Val Loss: 0.2521\n",
            "Epoch 4/250, Loss: 0.2637, Val Loss: 0.2472\n",
            "Epoch 5/250, Loss: 0.2348, Val Loss: 0.2823\n",
            "Epoch 6/250, Loss: 0.2131, Val Loss: 0.2350\n",
            "Epoch 7/250, Loss: 0.1986, Val Loss: 0.2345\n",
            "Epoch 8/250, Loss: 0.1818, Val Loss: 0.2368\n",
            "Epoch 9/250, Loss: 0.1681, Val Loss: 0.2427\n",
            "Epoch 10/250, Loss: 0.1594, Val Loss: 0.2565\n",
            "Epoch 11/250, Loss: 0.1472, Val Loss: 0.2475\n",
            "Epoch 12/250, Loss: 0.1408, Val Loss: 0.2590\n",
            "Epoch 13/250, Loss: 0.1308, Val Loss: 0.2620\n",
            "Epoch 14/250, Loss: 0.0977, Val Loss: 0.2773\n",
            "Epoch 15/250, Loss: 0.0885, Val Loss: 0.2909\n",
            "Epoch 16/250, Loss: 0.0841, Val Loss: 0.3017\n",
            "Epoch 17/250, Loss: 0.0751, Val Loss: 0.3087\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.07%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=Adam, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'Adam', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 0.5338, Val Loss: 0.3288\n",
            "Epoch 2/350, Loss: 0.3497, Val Loss: 0.2774\n",
            "Epoch 3/350, Loss: 0.2948, Val Loss: 0.2559\n",
            "Epoch 4/350, Loss: 0.2654, Val Loss: 0.2541\n",
            "Epoch 5/350, Loss: 0.2409, Val Loss: 0.2400\n",
            "Epoch 6/350, Loss: 0.2197, Val Loss: 0.2304\n",
            "Epoch 7/350, Loss: 0.1987, Val Loss: 0.2233\n",
            "Epoch 8/350, Loss: 0.1882, Val Loss: 0.2496\n",
            "Epoch 9/350, Loss: 0.1746, Val Loss: 0.2373\n",
            "Epoch 10/350, Loss: 0.1636, Val Loss: 0.2696\n",
            "Epoch 11/350, Loss: 0.1523, Val Loss: 0.2371\n",
            "Epoch 12/350, Loss: 0.1406, Val Loss: 0.2597\n",
            "Epoch 13/350, Loss: 0.1352, Val Loss: 0.2634\n",
            "Epoch 14/350, Loss: 0.1051, Val Loss: 0.2749\n",
            "Epoch 15/350, Loss: 0.0917, Val Loss: 0.2763\n",
            "Epoch 16/350, Loss: 0.0861, Val Loss: 0.2786\n",
            "Epoch 17/350, Loss: 0.0814, Val Loss: 0.2990\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.16%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=SGD, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'avg'}\n",
            "Epoch 1/5, Loss: 0.7097, Val Loss: 0.4289\n",
            "Epoch 2/5, Loss: 0.4580, Val Loss: 0.3636\n",
            "Epoch 3/5, Loss: 0.4016, Val Loss: 0.3336\n",
            "Epoch 4/5, Loss: 0.3548, Val Loss: 0.2986\n",
            "Epoch 5/5, Loss: 0.3273, Val Loss: 0.3104\n",
            "Accuracy: 88.25%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=SGD, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'avg'}\n",
            "Epoch 1/50, Loss: 0.7113, Val Loss: 0.4328\n",
            "Epoch 2/50, Loss: 0.4632, Val Loss: 0.3692\n",
            "Epoch 3/50, Loss: 0.3932, Val Loss: 0.3170\n",
            "Epoch 4/50, Loss: 0.3494, Val Loss: 0.2879\n",
            "Epoch 5/50, Loss: 0.3160, Val Loss: 0.2717\n",
            "Epoch 6/50, Loss: 0.2940, Val Loss: 0.2604\n",
            "Epoch 7/50, Loss: 0.2756, Val Loss: 0.2481\n",
            "Epoch 8/50, Loss: 0.2607, Val Loss: 0.2372\n",
            "Epoch 9/50, Loss: 0.2485, Val Loss: 0.2389\n",
            "Epoch 10/50, Loss: 0.2338, Val Loss: 0.2311\n",
            "Epoch 11/50, Loss: 0.2269, Val Loss: 0.2265\n",
            "Epoch 12/50, Loss: 0.2159, Val Loss: 0.2203\n",
            "Epoch 13/50, Loss: 0.2077, Val Loss: 0.2353\n",
            "Epoch 14/50, Loss: 0.2004, Val Loss: 0.2212\n",
            "Epoch 15/50, Loss: 0.1933, Val Loss: 0.2252\n",
            "Epoch 16/50, Loss: 0.1852, Val Loss: 0.2147\n",
            "Epoch 17/50, Loss: 0.1797, Val Loss: 0.2221\n",
            "Epoch 18/50, Loss: 0.1713, Val Loss: 0.2168\n",
            "Epoch 19/50, Loss: 0.1653, Val Loss: 0.2239\n",
            "Epoch 20/50, Loss: 0.1582, Val Loss: 0.2240\n",
            "Epoch 21/50, Loss: 0.1548, Val Loss: 0.2164\n",
            "Epoch 22/50, Loss: 0.1513, Val Loss: 0.2281\n",
            "Epoch 23/50, Loss: 0.1228, Val Loss: 0.2126\n",
            "Epoch 24/50, Loss: 0.1159, Val Loss: 0.2212\n",
            "Epoch 25/50, Loss: 0.1136, Val Loss: 0.2264\n",
            "Epoch 26/50, Loss: 0.1093, Val Loss: 0.2247\n",
            "Epoch 27/50, Loss: 0.1064, Val Loss: 0.2382\n",
            "Epoch 28/50, Loss: 0.1000, Val Loss: 0.2349\n",
            "Epoch 29/50, Loss: 0.0975, Val Loss: 0.2386\n",
            "Epoch 30/50, Loss: 0.0857, Val Loss: 0.2375\n",
            "Epoch 31/50, Loss: 0.0828, Val Loss: 0.2360\n",
            "Epoch 32/50, Loss: 0.0803, Val Loss: 0.2417\n",
            "Epoch 33/50, Loss: 0.0765, Val Loss: 0.2415\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.97%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=SGD, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'avg'}\n",
            "Epoch 1/100, Loss: 0.7390, Val Loss: 0.4355\n",
            "Epoch 2/100, Loss: 0.4800, Val Loss: 0.3609\n",
            "Epoch 3/100, Loss: 0.4023, Val Loss: 0.3205\n",
            "Epoch 4/100, Loss: 0.3578, Val Loss: 0.2992\n",
            "Epoch 5/100, Loss: 0.3257, Val Loss: 0.2760\n",
            "Epoch 6/100, Loss: 0.3010, Val Loss: 0.2641\n",
            "Epoch 7/100, Loss: 0.2842, Val Loss: 0.2463\n",
            "Epoch 8/100, Loss: 0.2671, Val Loss: 0.2486\n",
            "Epoch 9/100, Loss: 0.2520, Val Loss: 0.2497\n",
            "Epoch 10/100, Loss: 0.2432, Val Loss: 0.2431\n",
            "Epoch 11/100, Loss: 0.2337, Val Loss: 0.2331\n",
            "Epoch 12/100, Loss: 0.2228, Val Loss: 0.2220\n",
            "Epoch 13/100, Loss: 0.2154, Val Loss: 0.2229\n",
            "Epoch 14/100, Loss: 0.2082, Val Loss: 0.2230\n",
            "Epoch 15/100, Loss: 0.1990, Val Loss: 0.2248\n",
            "Epoch 16/100, Loss: 0.1932, Val Loss: 0.2155\n",
            "Epoch 17/100, Loss: 0.1836, Val Loss: 0.2188\n",
            "Epoch 18/100, Loss: 0.1754, Val Loss: 0.2119\n",
            "Epoch 19/100, Loss: 0.1704, Val Loss: 0.2254\n",
            "Epoch 20/100, Loss: 0.1674, Val Loss: 0.2139\n",
            "Epoch 21/100, Loss: 0.1581, Val Loss: 0.2173\n",
            "Epoch 22/100, Loss: 0.1526, Val Loss: 0.2203\n",
            "Epoch 23/100, Loss: 0.1472, Val Loss: 0.2252\n",
            "Epoch 24/100, Loss: 0.1462, Val Loss: 0.2299\n",
            "Epoch 25/100, Loss: 0.1195, Val Loss: 0.2157\n",
            "Epoch 26/100, Loss: 0.1107, Val Loss: 0.2125\n",
            "Epoch 27/100, Loss: 0.1060, Val Loss: 0.2237\n",
            "Epoch 28/100, Loss: 0.1028, Val Loss: 0.2211\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.54%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=SGD, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'avg'}\n",
            "Epoch 1/250, Loss: 0.7473, Val Loss: 0.4946\n",
            "Epoch 2/250, Loss: 0.4774, Val Loss: 0.3605\n",
            "Epoch 3/250, Loss: 0.4071, Val Loss: 0.3394\n",
            "Epoch 4/250, Loss: 0.3542, Val Loss: 0.3050\n",
            "Epoch 5/250, Loss: 0.3234, Val Loss: 0.2786\n",
            "Epoch 6/250, Loss: 0.2992, Val Loss: 0.2695\n",
            "Epoch 7/250, Loss: 0.2780, Val Loss: 0.2584\n",
            "Epoch 8/250, Loss: 0.2667, Val Loss: 0.2466\n",
            "Epoch 9/250, Loss: 0.2510, Val Loss: 0.2382\n",
            "Epoch 10/250, Loss: 0.2407, Val Loss: 0.2310\n",
            "Epoch 11/250, Loss: 0.2303, Val Loss: 0.2201\n",
            "Epoch 12/250, Loss: 0.2208, Val Loss: 0.2222\n",
            "Epoch 13/250, Loss: 0.2133, Val Loss: 0.2260\n",
            "Epoch 14/250, Loss: 0.2055, Val Loss: 0.2258\n",
            "Epoch 15/250, Loss: 0.1992, Val Loss: 0.2192\n",
            "Epoch 16/250, Loss: 0.1898, Val Loss: 0.2108\n",
            "Epoch 17/250, Loss: 0.1853, Val Loss: 0.2248\n",
            "Epoch 18/250, Loss: 0.1782, Val Loss: 0.2204\n",
            "Epoch 19/250, Loss: 0.1713, Val Loss: 0.2142\n",
            "Epoch 20/250, Loss: 0.1674, Val Loss: 0.2110\n",
            "Epoch 21/250, Loss: 0.1622, Val Loss: 0.2205\n",
            "Epoch 22/250, Loss: 0.1536, Val Loss: 0.2286\n",
            "Epoch 23/250, Loss: 0.1291, Val Loss: 0.2047\n",
            "Epoch 24/250, Loss: 0.1224, Val Loss: 0.2225\n",
            "Epoch 25/250, Loss: 0.1173, Val Loss: 0.2244\n",
            "Epoch 26/250, Loss: 0.1150, Val Loss: 0.2220\n",
            "Epoch 27/250, Loss: 0.1121, Val Loss: 0.2219\n",
            "Epoch 28/250, Loss: 0.1085, Val Loss: 0.2243\n",
            "Epoch 29/250, Loss: 0.1030, Val Loss: 0.2349\n",
            "Epoch 30/250, Loss: 0.0912, Val Loss: 0.2336\n",
            "Epoch 31/250, Loss: 0.0881, Val Loss: 0.2329\n",
            "Epoch 32/250, Loss: 0.0837, Val Loss: 0.2316\n",
            "Epoch 33/250, Loss: 0.0829, Val Loss: 0.2380\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.74%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=SGD, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'avg'}\n",
            "Epoch 1/350, Loss: 0.7613, Val Loss: 0.4488\n",
            "Epoch 2/350, Loss: 0.4892, Val Loss: 0.3690\n",
            "Epoch 3/350, Loss: 0.4159, Val Loss: 0.3247\n",
            "Epoch 4/350, Loss: 0.3721, Val Loss: 0.3251\n",
            "Epoch 5/350, Loss: 0.3373, Val Loss: 0.3052\n",
            "Epoch 6/350, Loss: 0.3137, Val Loss: 0.2708\n",
            "Epoch 7/350, Loss: 0.2964, Val Loss: 0.2580\n",
            "Epoch 8/350, Loss: 0.2781, Val Loss: 0.2444\n",
            "Epoch 9/350, Loss: 0.2605, Val Loss: 0.2431\n",
            "Epoch 10/350, Loss: 0.2514, Val Loss: 0.2433\n",
            "Epoch 11/350, Loss: 0.2400, Val Loss: 0.2368\n",
            "Epoch 12/350, Loss: 0.2336, Val Loss: 0.2323\n",
            "Epoch 13/350, Loss: 0.2219, Val Loss: 0.2276\n",
            "Epoch 14/350, Loss: 0.2153, Val Loss: 0.2257\n",
            "Epoch 15/350, Loss: 0.2103, Val Loss: 0.2306\n",
            "Epoch 16/350, Loss: 0.2003, Val Loss: 0.2267\n",
            "Epoch 17/350, Loss: 0.1952, Val Loss: 0.2217\n",
            "Epoch 18/350, Loss: 0.1866, Val Loss: 0.2222\n",
            "Epoch 19/350, Loss: 0.1809, Val Loss: 0.2121\n",
            "Epoch 20/350, Loss: 0.1761, Val Loss: 0.2269\n",
            "Epoch 21/350, Loss: 0.1703, Val Loss: 0.2216\n",
            "Epoch 22/350, Loss: 0.1630, Val Loss: 0.2239\n",
            "Epoch 23/350, Loss: 0.1591, Val Loss: 0.2212\n",
            "Epoch 24/350, Loss: 0.1512, Val Loss: 0.2185\n",
            "Epoch 25/350, Loss: 0.1486, Val Loss: 0.2223\n",
            "Epoch 26/350, Loss: 0.1227, Val Loss: 0.2194\n",
            "Epoch 27/350, Loss: 0.1168, Val Loss: 0.2224\n",
            "Epoch 28/350, Loss: 0.1109, Val Loss: 0.2231\n",
            "Epoch 29/350, Loss: 0.1084, Val Loss: 0.2251\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.41%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=RMSProp, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'avg'}\n",
            "Epoch 1/5, Loss: 1.8006, Val Loss: 0.5123\n",
            "Epoch 2/5, Loss: 0.5436, Val Loss: 0.4086\n",
            "Epoch 3/5, Loss: 0.4825, Val Loss: 0.3911\n",
            "Epoch 4/5, Loss: 0.4574, Val Loss: 0.3589\n",
            "Epoch 5/5, Loss: 0.4421, Val Loss: 0.3562\n",
            "Accuracy: 86.40%\n",
            "Running experiment with kernel_size=3, pooling_type=avg, optimizer=RMSProp, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'avg'}\n",
            "Epoch 1/50, Loss: 1.5893, Val Loss: 0.4573\n",
            "Epoch 2/50, Loss: 0.5277, Val Loss: 0.4170\n",
            "Epoch 3/50, Loss: 0.4767, Val Loss: 0.3821\n",
            "Epoch 4/50, Loss: 0.5022, Val Loss: 0.4059\n",
            "Epoch 5/50, Loss: 0.4762, Val Loss: 0.3745\n",
            "Epoch 6/50, Loss: 0.4239, Val Loss: 0.3673\n",
            "Epoch 7/50, Loss: 0.4336, Val Loss: 0.3575\n",
            "Epoch 8/50, Loss: 0.4209, Val Loss: 0.3650\n",
            "Epoch 9/50, Loss: 0.4164, Val Loss: 0.3533\n",
            "Epoch 10/50, Loss: 0.4132, Val Loss: 0.3419\n",
            "Epoch 11/50, Loss: 0.4896, Val Loss: 0.3695\n",
            "Epoch 12/50, Loss: 0.4143, Val Loss: 0.4008\n",
            "Epoch 13/50, Loss: 0.4119, Val Loss: 0.3733\n",
            "Epoch 14/50, Loss: 0.4160, Val Loss: 0.3891\n",
            "Epoch 15/50, Loss: 0.4701, Val Loss: 0.3794\n",
            "Epoch 16/50, Loss: 0.4302, Val Loss: 0.3728\n",
            "Epoch 17/50, Loss: 0.3578, Val Loss: 0.3310\n",
            "Epoch 18/50, Loss: 0.3512, Val Loss: 0.3563\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-481af5ea2ae4>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running experiment with kernel_size={kernel_size}, pooling_type={pooling_type}, optimizer={optimizer}, epochs={epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-481af5ea2ae4>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(kernel_size, pooling_type, optimizer_type, epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-cccfb845f51a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}